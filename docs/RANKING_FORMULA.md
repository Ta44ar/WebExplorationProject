# Ranking Formula Documentation

## Overview

The ranking system evaluates web pages based on their credibility and relevance to the search query. The system uses a weighted multi-criteria approach combining four main factors.

## Formula

```
TotalScore = w? × PositionScore + w? × ReferenceScore + w? × SpecialistScore + w? × CredibilityScore
```

### Default Weights

| Weight | Criterion | Default Value | Justification |
|--------|-----------|---------------|---------------|
| w? | Position Score | 0.20 | Search engines have already done initial relevance ranking |
| w? | Reference Score | 0.25 | Quality sources cite other reliable sources |
| w? | Specialist Score | 0.30 | Expert terminology indicates domain knowledge |
| w? | Credibility Score | 0.25 | Absence of manipulation indicates trustworthiness |

**Total: 1.00**

---

## Criteria Details

### 1. Position Score (w? = 0.20)

**Purpose:** Leverage search engine's initial ranking as a baseline.

**Formula:**
```
PositionScore = (N - position + 1) / N
```
Where:
- `N` = total number of pages analyzed
- `position` = page's position in search results (1-based)

**Result:** Linear scale from 1.0 (first position) to ~0 (last position)

**Justification:**
- Search engines use sophisticated algorithms for relevance
- However, SEO manipulation is possible, so weight is limited to 20%
- Provides baseline ordering while allowing other criteria to adjust

---

### 2. Reference Score (w? = 0.25)

**Purpose:** Evaluate the quality and diversity of external references.

**Components:**
1. **Outbound Link Count** - Number of links to external sources
2. **Unique Domains Referenced** - Diversity of cited sources

**Formula:**
```
LinkScore = min(1.0, OutboundLinks / MaxExpectedLinks)
DomainScore = min(1.0, UniqueDomains / MaxExpectedDomains)
ReferenceScore = 0.4 × LinkScore + 0.6 × DomainScore
```

**Default Parameters:**
- `MaxExpectedLinks` = 50
- `MaxExpectedDomains` = 20

**Justification:**
- Credible articles cite diverse, authoritative sources
- Academic and medical content typically references multiple studies
- Domain diversity indicates broader research, not just echo chamber
- Higher weight on diversity (0.6) vs quantity (0.4) to prevent gaming

---

### 3. Specialist Score (w? = 0.30)

**Purpose:** Measure domain expertise through specialized terminology.

**Formula:**
```
TermDensity = SpecialistTermCount / TotalWordCount
SpecialistScore = min(1.0, TermDensity / TargetDensity)
```

**Default Parameters:**
- `TargetDensity` = 0.05 (5% of words should be specialist terms)

**Dictionary Sources:**
- **Default dictionaries** - Universal scientific/academic terms (bilingual EN/PL)
- **AI-generated dictionaries** - Topic-specific terms generated by OpenAI (1 request)

**Default Specialist Terms (universal):**
- peer-review, meta-analysis, systematic review, randomized, control group
- statistical significance, methodology, hypothesis, correlation, causation
- evidence-based, reproducibility, empirical, longitudinal study...

**Justification:**
- Expert content uses precise, domain-specific vocabulary
- Density approach prevents gaming by keyword stuffing
- Universal terms work across any topic
- AI generation provides topic-specific enhancement
- Highest weight (0.30) as expertise is primary credibility indicator

---

### 4. Credibility Score (w? = 0.25)

**Purpose:** Detect emotional manipulation and propaganda techniques.

**Components:**
1. **Emotional Words** - Loaded, sensational language
2. **Propaganda Phrases** - Known manipulation patterns

**Rule-Based Calculation:**
```
EmotionalRatio = EmotionalWordCount / TotalWordCount
PropagandaRatio = PropagandaPhraseCount × 5 / TotalWordCount
CredibilityScore = max(0, 1.0 - EmotionalRatio × 20 - PropagandaRatio × 10)
```

**Default Emotional Words (universal):**
- shocking, unbelievable, scandal, disaster, terrifying, alarming
- breaking, exclusive, bombshell, controversial, extreme...

**Default Propaganda Phrases (universal):**
- "they don't want you to know", "hidden truth", "wake up"
- "do your own research", "mainstream media lies", "follow the money"
- "everyone knows", "censored", "suppressed"...

**Justification:**
- Credible sources use neutral, factual language
- Emotional manipulation indicates potential misinformation
- Propaganda techniques are well-documented and detectable
- Universal patterns work across any topic

---

## Configuration

### appsettings.json

```json
{
  "Ranking": {
    "PositionWeight": 0.20,
    "ReferenceWeight": 0.25,
    "SpecialistWeight": 0.30,
    "EmotionWeight": 0.25,
    "GenerateDictionaries": true,
    "AiModel": "gpt-4o-mini"
  }
}
```

### Custom Weights

Weights can be adjusted based on use case:

| Use Case | Position | Reference | Specialist | Credibility |
|----------|----------|-----------|------------|-------------|
| Medical Content | 0.15 | 0.25 | 0.35 | 0.25 |
| News Articles | 0.20 | 0.30 | 0.20 | 0.30 |
| Academic Papers | 0.10 | 0.30 | 0.40 | 0.20 |

---

## Output Files

### {Source}_ranking.csv
Main ranking results with all scores:
```
FinalRank,Url,Title,TotalScore,PositionScore,ReferenceScore,SpecialistScore,CredibilityScore,...
```

### {Source}_ranking_details.txt
Human-readable report with:
- Formula explanation
- Weight configuration
- Top 10 detailed analysis

### Combined_ranking.csv
Merged ranking from all sources (duplicates removed, highest score kept)

---

## Limitations

1. **Language Dependency** - Default dictionaries are Polish/English
2. **Topic Specificity** - Best with AI-generated dictionaries for any topic
3. **AI Costs** - Dictionary generation costs ~$0.001 (1 request)
4. **Temporal Validity** - Propaganda techniques evolve over time
5. **Context Blindness** - Rule-based detection may miss subtle manipulation

---

## Future Improvements

1. **Machine Learning Model** - Train custom classifier on labeled data
2. **Source Authority Database** - Integrate known reliable source lists
3. **Fact-Checking API** - Cross-reference claims with fact-checkers
4. **Multi-language Support** - Extend default term dictionaries
5. **User Feedback Loop** - Allow rating corrections for model improvement
